{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import pandas as pd\n",
    "import classifiers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T07:52:08.305397Z",
     "start_time": "2024-04-23T07:52:05.329231100Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compare_plain_vote(dataset_df):\n",
    "    \"\"\"\n",
    "    Train fcnn on specified dataset, and compute accuracy of sample and track prediction on test set.\n",
    "    :param dataset_df: the complete dataset dataframe, which will be splitted accordingly\n",
    "    :return: the accuracy of by sample prediction and by track prediction\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_val, y_val = utils.train_val_split(dataset_df, keep_trackID=True)\n",
    "    y_val_pred = classifiers.tensorflow_fcnn(X_train, y_train, X_val, y_val.GenreID, verbose=False)\n",
    "    df_sample = pd.DataFrame({\"pred\": y_val_pred, \"track\": y_val.TrackID, \"true\": y_val.GenreID})\n",
    "    df_track = df_sample.groupby('track').agg(list)\n",
    "    df_track['pred_by_track'] = \\\n",
    "        df_track['pred'] \\\n",
    "        .apply(lambda x: np.unique(x, return_counts=True)) \\\n",
    "        .apply(lambda x: x[0][np.argmax(x[1])])\n",
    "    df_track['true_by_track'] = \\\n",
    "        df_track['true'] \\\n",
    "        .apply(lambda x: np.unique(x, return_counts=True)) \\\n",
    "        .apply(lambda x: x[0][np.argmax(x[1])])\n",
    "    return np.mean(df_sample.pred == df_sample.true), np.mean(df_track.pred_by_track == df_track.true_by_track)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T07:52:08.321818300Z",
     "start_time": "2024-04-23T07:52:08.310374100Z"
    }
   },
   "id": "d47803316ec4f1a8",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df5s, df10s, df30s = utils.task4_df()\n",
    "dataset = {\n",
    "    \"5s\" : df5s,\n",
    "    # \"10s\" : df10s,\n",
    "    \"30s\" : df30s,\n",
    "    # \"whole\" : pd.concat((df5s, df10s, df30s), ignore_index=True)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T07:52:08.432501300Z",
     "start_time": "2024-04-23T07:52:08.321818300Z"
    }
   },
   "id": "625c8cf240e661b4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, df in dataset.items():\n",
    "    results[name] = []\n",
    "    for i in range(10):\n",
    "        results[name] += compare_plain_vote(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T07:53:20.718552900Z",
     "start_time": "2024-04-23T07:52:08.431502400Z"
    }
   },
   "id": "99a5d31f16bf2945",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean gap : {'5s': 0.05, '30s': 0.0}\n",
      "Plain accuracy mean : {'5s': 0.68, '30s': 0.73}\n",
      "Vote accuracy mean : {'5s': 0.74, '30s': 0.73}\n",
      "Plain accuracy std : {'5s': 0.01, '30s': 0.027}\n",
      "Vote accuracy std : {'5s': 0.021, '30s': 0.027}\n"
     ]
    }
   ],
   "source": [
    "# Group the (vote, no vote) into tuples\n",
    "grouped = {name : list(zip(*[iter(values)]*2)) for name, values in results.items()}\n",
    "\n",
    "# Take the mean of accuracies of models with voted genre \n",
    "vote_acc_mean = \\\n",
    "    {name : np.round(np.mean([v[1] for v in val]), decimals=2) for name, val in grouped.items()}\n",
    "# Take the standard deviation of accuracies of models with voted genre \n",
    "vote_acc_std = \\\n",
    "    {name : np.round(np.sqrt(np.mean([v[1]**2 for v in val]) - np.mean([v[1] for v in val])**2), decimals=3) for name, val in grouped.items()}\n",
    "\n",
    "# Take the mean of accuracies of models without voted genre \n",
    "plain_acc_mean = \\\n",
    "    {name : np.round(np.mean([v[0] for v in val]), decimals=2) for name, val in grouped.items()}\n",
    "# Take the std of accuracies of models without voted genre \n",
    "plain_acc_std = \\\n",
    "    {name : np.round(np.sqrt(np.mean([v[0]**2 for v in val]) - np.mean([v[0] for v in val])**2), decimals=3) for name, val in grouped.items()}\n",
    "            \n",
    "\n",
    "mean_gap = {name :np.round(np.mean([v-p for p,v in val]), decimals=2) for name, val in grouped.items()}\n",
    "\n",
    "print(\n",
    "    f\"Mean gap : {mean_gap}\\n\"\n",
    "    f\"Plain accuracy mean : {plain_acc_mean}\\n\"\n",
    "    f\"Vote accuracy mean : {vote_acc_mean}\\n\"\n",
    "    f\"Plain accuracy std : {plain_acc_std}\\n\"\n",
    "    f\"Vote accuracy std : {vote_acc_std}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T07:53:20.734310600Z",
     "start_time": "2024-04-23T07:53:20.723486Z"
    }
   },
   "id": "d5a0fffd9e4d33c7",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
